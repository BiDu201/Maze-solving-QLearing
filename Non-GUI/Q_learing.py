# -*- coding: utf-8 -*-
"""2001200657_DuongTrongBinh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hWtvddWCBp0M7ARxT5bdkj0uduIdA70y
"""

import numpy as np

# Khởi tạo các biến và hằng số
MAZE_WIDTH = 10  # Số ô vuông theo chiều ngang
MAZE_HEIGHT = 10  # Số ô vuông theo chiều dọc

maze = np.array([
    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
    [0, 1, 0, 1, 1, 1, 1, 1, 1, 0],
    [0, 1, 1, 1, 0, 0, 0, 0, 1, 0],
    [0, 1, 0, 1, 1, 1, 1, 0, 1, 0],
    [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
    [0, 1, 0, 0, 0, 0, 1, 0, 1, 0],
    [0, 1, 0, 1, 1, 1, 1, 0, 1, 0],
    [0, 1, 0, 1, 0, 0, 0, 0, 1, 0],
    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],
    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
])

start_pos = (0, 6)  # Vị trí bắt đầu
end_pos = (9, 4)  # Vị trí kết thúc

q_table = np.zeros((MAZE_HEIGHT, MAZE_WIDTH, 4))  # Bảng Q

actions = {
    0: (-1, 0),  # Di chuyển lên
    1: (1, 0),  # Di chuyển xuống
    2: (0, -1),  # Di chuyển sang trái
    3: (0, 1)  # Di chuyển sang phải
}


def get_valid_actions(position):
    valid_actions = []
    for action, (dx, dy) in actions.items():
        new_x = position[0] + dx
        new_y = position[1] + dy
        if 0 <= new_x < MAZE_HEIGHT and 0 <= new_y < MAZE_WIDTH and maze[new_x, new_y] == 1:
            valid_actions.append(action)
    return valid_actions


def update_q_table(position, action, next_position, reward):
    alpha = 0.1  # Tốc độ học
    gamma = 0.9  # Hệ số giảm
    q_table[position[0], position[1], action] = (1 - alpha) * q_table[position[0], position[1], action] + alpha * (
                reward + gamma * np.max(q_table[next_position]))


def choose_action(position, epsilon):
    if np.random.uniform() < epsilon:
        return np.random.choice(get_valid_actions(position))
    else:
        return np.argmax(q_table[position[0], position[1]])


def move(position, action):
    dx, dy = actions[action]
    new_x = position[0] + dx
    new_y = position[1] + dy
    if 0 <= new_x < MAZE_HEIGHT and 0 <= new_y < MAZE_WIDTH and maze[new_x, new_y] == 1:
        return new_x, new_y
    return position


def run_q_learning():
    episodes = 100  # Số lượng tập huấn luyện
    max_steps = 100  # Số lượng bước tối đa trong mỗi tập
    epsilon = 0.9  # Tham số epsilon trong thuật toán ε-greedy

    for episode in range(episodes):
        path = []
        position = start_pos
        path.append(start_pos)

        for step in range(max_steps):
            action = choose_action(position, epsilon) # Lấy hành động
            next_position = move(position, action)
            reward = 1 if next_position == end_pos else 0

            update_q_table(position, action, next_position, reward)            

            position = next_position

            path.append(position)

            if position == end_pos:
                break
           
        # In ra trạng thái di chuyển của mỗi tập
        print("Episode:", episode + 1)
        print("Path:", path)

# Chạy thuật toán Q-learning
run_q_learning()